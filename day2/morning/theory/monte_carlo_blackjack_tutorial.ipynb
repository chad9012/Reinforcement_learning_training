{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Methods in Reinforcement Learning\n",
        "## Applied to Blackjack Environment\n",
        "\n",
        "This notebook demonstrates Monte Carlo methods for reinforcement learning using the Blackjack environment from Gymnasium.\n",
        "\n",
        "### What we'll cover:\n",
        "1. Monte Carlo Evaluation (Prediction)\n",
        "2. Monte Carlo Control\n",
        "3. Why we estimate Q-values instead of V-values\n",
        "4. Transforming MDP to MRP\n",
        "5. Constant-Œ± Monte Carlo implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monte Carlo Methods Theory\n",
        "\n",
        "### What are Monte Carlo Methods?\n",
        "Monte Carlo methods learn value functions and optimal policies from **complete episodes** of experience. Unlike dynamic programming, they don't require knowledge of the environment's dynamics.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Model-free**: No need to know transition probabilities\n",
        "- **Episode-based**: Learn from complete episodes\n",
        "- **Sample-based**: Use actual returns rather than expected returns\n",
        "\n",
        "### Monte Carlo Evaluation (Prediction)\n",
        "Goal: Estimate the value function V^œÄ(s) for a given policy œÄ\n",
        "\n",
        "**Algorithm:**\n",
        "1. Generate episodes following policy œÄ\n",
        "2. For each state visited, calculate the return G_t\n",
        "3. Average returns to estimate V(s)\n",
        "\n",
        "### Monte Carlo Control\n",
        "Goal: Find the optimal policy œÄ*\n",
        "\n",
        "**Algorithm:**\n",
        "1. Policy Evaluation: Estimate Q^œÄ(s,a)\n",
        "2. Policy Improvement: Update policy greedily\n",
        "3. Repeat until convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Estimate Q-values instead of V-values?\n",
        "\n",
        "### The Problem with V-values in Model-free Settings\n",
        "\n",
        "In model-free reinforcement learning, we don't know the environment dynamics P(s',r|s,a). \n",
        "\n",
        "**Policy Improvement with V-values requires:**\n",
        "```\n",
        "œÄ(s) = argmax_a Œ£_{s',r} P(s',r|s,a)[r + Œ≥V(s')]\n",
        "```\n",
        "\n",
        "**Problem:** We don't know P(s',r|s,a)!\n",
        "\n",
        "### Solution: Use Q-values\n",
        "\n",
        "**Policy Improvement with Q-values:**\n",
        "```\n",
        "œÄ(s) = argmax_a Q(s,a)\n",
        "```\n",
        "\n",
        "**Advantage:** No need for environment model!\n",
        "\n",
        "### MDP to MRP Transformation\n",
        "\n",
        "When we fix a policy œÄ, we transform the **Markov Decision Process (MDP)** into a **Markov Reward Process (MRP)**:\n",
        "\n",
        "**MDP:** (S, A, P, R, Œ≥)\n",
        "**MRP:** (S, P^œÄ, R^œÄ, Œ≥)\n",
        "\n",
        "Where:\n",
        "- P^œÄ(s'|s) = Œ£_a œÄ(a|s)P(s'|s,a)\n",
        "- R^œÄ(s) = Œ£_a œÄ(a|s)R(s,a)\n",
        "\n",
        "This transformation allows us to evaluate the value function for a fixed policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create Blackjack environment\n",
        "env = gym.make('Blackjack-v1', sab=True)  # sab=True for state-action-based rewards\n",
        "\n",
        "print(\"Blackjack Environment:\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(\"\\nState representation: (player_sum, dealer_card, usable_ace)\")\n",
        "print(\"Actions: 0 = Stick, 1 = Hit\")\n",
        "\n",
        "# Test environment\n",
        "state, info = env.reset()\n",
        "print(f\"\\nSample initial state: {state}\")\n",
        "print(f\"Player sum: {state[0]}, Dealer card: {state[1]}, Usable ace: {state[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def generate_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Generate a complete episode following the given policy\n",
        "    Returns: list of (state, action, reward) tuples\n",
        "    \"\"\"\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    \n",
        "    while True:\n",
        "        action = policy(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        state = next_state\n",
        "    \n",
        "    return episode\n",
        "\n",
        "def simple_policy(state):\n",
        "    \"\"\"\n",
        "    Simple policy: Hit if player sum < 20, else stick\n",
        "    \"\"\"\n",
        "    player_sum = state[0]\n",
        "    return 1 if player_sum < 20 else 0\n",
        "\n",
        "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy policy for exploration\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice([0, 1])  # Random action\n",
        "    else:\n",
        "        return 0 if Q[state][0] > Q[state][1] else 1  # Greedy action\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def monte_carlo_evaluation(env, policy, num_episodes=10000, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Monte Carlo evaluation to estimate V^œÄ(s)\n",
        "    \"\"\"\n",
        "    # Initialize\n",
        "    V = defaultdict(float)\n",
        "    returns = defaultdict(list)\n",
        "    \n",
        "    print(f\"Running Monte Carlo Evaluation for {num_episodes} episodes...\")\n",
        "    \n",
        "    for episode_num in tqdm(range(num_episodes)):\n",
        "        # Generate episode\n",
        "        episode = generate_episode(env, policy)\n",
        "        \n",
        "        # Calculate returns for each state\n",
        "        G = 0\n",
        "        visited_states = set()\n",
        "        \n",
        "        # Work backwards through episode\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            \n",
        "            # First-visit MC\n",
        "            if state not in visited_states:\n",
        "                returns[state].append(G)\n",
        "                V[state] = np.mean(returns[state])\n",
        "                visited_states.add(state)\n",
        "    \n",
        "    return dict(V)\n",
        "\n",
        "# Run Monte Carlo Evaluation\n",
        "V_simple = monte_carlo_evaluation(env, simple_policy, num_episodes=50000)\n",
        "print(f\"\\nEstimated {len(V_simple)} state values\")\n",
        "print(\"Sample state values:\")\n",
        "for i, (state, value) in enumerate(list(V_simple.items())[:5]):\n",
        "    print(f\"State {state}: V = {value:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def constant_alpha_monte_carlo_control(env, num_episodes=100000, alpha=0.01, epsilon=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Constant-Œ± Monte Carlo Control\n",
        "    Uses incremental updates instead of averaging all returns\n",
        "    \"\"\"\n",
        "    # Initialize Q-values\n",
        "    Q = defaultdict(lambda: np.zeros(2))  # 2 actions: stick(0), hit(1)\n",
        "    \n",
        "    print(f\"Running Constant-Œ± Monte Carlo Control...\")\n",
        "    print(f\"Episodes: {num_episodes}, Œ±: {alpha}, Œµ: {epsilon}\")\n",
        "    \n",
        "    episode_rewards = []\n",
        "    \n",
        "    for episode_num in tqdm(range(num_episodes)):\n",
        "        # Generate episode using Œµ-greedy policy\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        # Generate episode\n",
        "        while True:\n",
        "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            episode_reward += reward\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "            state = next_state\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        \n",
        "        # Update Q-values using constant-Œ±\n",
        "        G = 0\n",
        "        visited_pairs = set()\n",
        "        \n",
        "        # Work backwards through episode\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            \n",
        "            # First-visit update\n",
        "            if (state, action) not in visited_pairs:\n",
        "                # Constant-Œ± update: Q(s,a) ‚Üê Q(s,a) + Œ±[G - Q(s,a)]\n",
        "                Q[state][action] += alpha * (G - Q[state][action])\n",
        "                visited_pairs.add((state, action))\n",
        "    \n",
        "    return dict(Q), episode_rewards\n",
        "\n",
        "# Run Constant-Œ± Monte Carlo Control\n",
        "Q_optimal, rewards = constant_alpha_monte_carlo_control(\n",
        "    env, \n",
        "    num_episodes=100000, \n",
        "    alpha=0.01, \n",
        "    epsilon=0.1\n",
        ")\n",
        "\n",
        "print(f\"\\nLearned Q-values for {len(Q_optimal)} states\")\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def extract_policy(Q):\n",
        "    \"\"\"\n",
        "    Extract greedy policy from Q-values\n",
        "    \"\"\"\n",
        "    policy = {}\n",
        "    for state in Q:\n",
        "        policy[state] = np.argmax(Q[state])\n",
        "    return policy\n",
        "\n",
        "def policy_function(state, policy_dict):\n",
        "    \"\"\"\n",
        "    Policy function that can be used with generate_episode\n",
        "    \"\"\"\n",
        "    if state in policy_dict:\n",
        "        return policy_dict[state]\n",
        "    else:\n",
        "        # Default action for unseen states\n",
        "        return 0  # Stick\n",
        "\n",
        "# Extract optimal policy\n",
        "optimal_policy_dict = extract_policy(Q_optimal)\n",
        "print(f\"Optimal policy extracted for {len(optimal_policy_dict)} states\")\n",
        "\n",
        "# Create policy function\n",
        "def optimal_policy(state):\n",
        "    return policy_function(state, optimal_policy_dict)\n",
        "\n",
        "print(\"Optimal policy function created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def visualize_policy_interactive(policy_dict, title=\"Policy Visualization\"):\n",
        "    \"\"\"\n",
        "    Interactive visualization of the policy\n",
        "    \"\"\"\n",
        "    print(f\"\\n{title}\")\n",
        "    print(\"=\" * len(title))\n",
        "    \n",
        "    # Organize states by usable ace\n",
        "    no_ace_states = {}\n",
        "    ace_states = {}\n",
        "    \n",
        "    for state, action in policy_dict.items():\n",
        "        player_sum, dealer_card, usable_ace = state\n",
        "        if usable_ace:\n",
        "            ace_states[(player_sum, dealer_card)] = action\n",
        "        else:\n",
        "            no_ace_states[(player_sum, dealer_card)] = action\n",
        "    \n",
        "    def print_policy_table(states_dict, title):\n",
        "        print(f\"\\n{title}:\")\n",
        "        print(\"Player\\\\Dealer\", end=\"\")\n",
        "        for dealer in range(1, 11):\n",
        "            print(f\"{dealer:>4}\", end=\"\")\n",
        "        print()\n",
        "        \n",
        "        for player in range(12, 22):\n",
        "            print(f\"{player:>11}\", end=\"\")\n",
        "            for dealer in range(1, 11):\n",
        "                if (player, dealer) in states_dict:\n",
        "                    action = states_dict[(player, dealer)]\n",
        "                    symbol = \"H\" if action == 1 else \"S\"\n",
        "                    print(f\"{symbol:>4}\", end=\"\")\n",
        "                else:\n",
        "                    print(f\"-{4}\", end=\"\")\n",
        "            print()\n",
        "    \n",
        "    print_policy_table(no_ace_states, \"No Usable Ace\")\n",
        "    print_policy_table(ace_states, \"Usable Ace\")\n",
        "    \n",
        "    print(\"\\nLegend: H = Hit, S = Stick\")\n",
        "\n",
        "# Visualize the optimal policy\n",
        "visualize_policy_interactive(optimal_policy_dict, \"Optimal Policy from Monte Carlo Control\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def test_policy(env, policy, num_episodes=10000):\n",
        "    \"\"\"\n",
        "    Test a policy and return average reward\n",
        "    \"\"\"\n",
        "    total_reward = 0\n",
        "    wins = 0\n",
        "    losses = 0\n",
        "    draws = 0\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env, policy)\n",
        "        episode_reward = sum([reward for _, _, reward in episode])\n",
        "        total_reward += episode_reward\n",
        "        \n",
        "        if episode_reward > 0:\n",
        "            wins += 1\n",
        "        elif episode_reward < 0:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "    \n",
        "    avg_reward = total_reward / num_episodes\n",
        "    win_rate = wins / num_episodes\n",
        "    \n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'win_rate': win_rate,\n",
        "        'wins': wins,\n",
        "        'losses': losses,\n",
        "        'draws': draws\n",
        "    }\n",
        "\n",
        "# Test different policies\n",
        "print(\"Testing Policies...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test simple policy\n",
        "simple_results = test_policy(env, simple_policy, 10000)\n",
        "print(\"\\nSimple Policy (Hit if sum < 20):\")\n",
        "print(f\"Average Reward: {simple_results['avg_reward']:.4f}\")\n",
        "print(f\"Win Rate: {simple_results['win_rate']:.4f}\")\n",
        "print(f\"Wins: {simple_results['wins']}, Losses: {simple_results['losses']}, Draws: {simple_results['draws']}\")\n",
        "\n",
        "# Test optimal policy\n",
        "optimal_results = test_policy(env, optimal_policy, 10000)\n",
        "print(\"\\nOptimal Policy (Monte Carlo):\")\n",
        "print(f\"Average Reward: {optimal_results['avg_reward']:.4f}\")\n",
        "print(f\"Win Rate: {optimal_results['win_rate']:.4f}\")\n",
        "print(f\"Wins: {optimal_results['wins']}, Losses: {optimal_results['losses']}, Draws: {optimal_results['draws']}\")\n",
        "\n",
        "improvement = optimal_results['avg_reward'] - simple_results['avg_reward']\n",
        "print(f\"\\nImprovement: {improvement:.4f} ({improvement/abs(simple_results['avg_reward'])*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot learning progress\n",
        "def plot_learning_progress(rewards, window_size=1000):\n",
        "    \"\"\"\n",
        "    Plot the learning progress over episodes\n",
        "    \"\"\"\n",
        "    # Calculate moving average\n",
        "    moving_avg = []\n",
        "    for i in range(len(rewards)):\n",
        "        start_idx = max(0, i - window_size + 1)\n",
        "        moving_avg.append(np.mean(rewards[start_idx:i+1]))\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Plot raw rewards (sampled for visibility)\n",
        "    sample_indices = np.arange(0, len(rewards), max(1, len(rewards)//1000))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(sample_indices, [rewards[i] for i in sample_indices], alpha=0.3, color='lightblue')\n",
        "    plt.plot(moving_avg, color='red', linewidth=2, label=f'Moving Average (window={window_size})')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Episode Reward')\n",
        "    plt.title('Learning Progress: Episode Rewards')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot moving average only\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(moving_avg, color='red', linewidth=2)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Learning Progress: Moving Average')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Final average reward (last {window_size} episodes): {np.mean(rewards[-window_size:]):.4f}\")\n",
        "\n",
        "# Plot the learning progress\n",
        "plot_learning_progress(rewards, window_size=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def interactive_policy_explorer(Q_values):\n",
        "    \"\"\"\n",
        "    Interactive exploration of the learned policy\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERACTIVE POLICY EXPLORER\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    def explore_state(player_sum, dealer_card, usable_ace):\n",
        "        state = (player_sum, dealer_card, usable_ace)\n",
        "        \n",
        "        if state in Q_values:\n",
        "            q_stick = Q_values[state][0]\n",
        "            q_hit = Q_values[state][1]\n",
        "            optimal_action = \"STICK\" if q_stick > q_hit else \"HIT\"\n",
        "            \n",
        "            print(f\"\\nState: Player={player_sum}, Dealer={dealer_card}, Ace={'Yes' if usable_ace else 'No'}\")\n",
        "            print(f\"Q(s, STICK) = {q_stick:.4f}\")\n",
        "            print(f\"Q(s, HIT)   = {q_hit:.4f}\")\n",
        "            print(f\"Optimal Action: {optimal_action}\")\n",
        "            print(f\"Advantage: {abs(q_stick - q_hit):.4f}\")\n",
        "        else:\n",
        "            print(f\"\\nState not visited during training: Player={player_sum}, Dealer={dealer_card}, Ace={'Yes' if usable_ace else 'No'}\")\n",
        "    \n",
        "    # Example explorations\n",
        "    print(\"\\nExploring some interesting states:\")\n",
        "    \n",
        "    interesting_states = [\n",
        "        (20, 10, False),  # Strong hand vs strong dealer\n",
        "        (16, 10, False),  # Weak hand vs strong dealer\n",
        "        (12, 6, False),   # Weak hand vs weak dealer\n",
        "        (18, 9, True),    # Soft 18 vs 9\n",
        "        (13, 2, True),    # Soft 13 vs 2\n",
        "        (11, 10, False),  # 11 vs 10\n",
        "    ]\n",
        "    \n",
        "    for state in interesting_states:\n",
        "        explore_state(*state)\n",
        "    \n",
        "    return explore_state\n",
        "\n",
        "# Run interactive explorer\n",
        "explore_function = interactive_policy_explorer(Q_optimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monte Carlo Method Analysis\n",
        "\n",
        "### Key Insights from Our Implementation:\n",
        "\n",
        "#### 1. **Constant-Œ± vs Sample Average**\n",
        "- **Sample Average**: Q(s,a) = average of all returns\n",
        "- **Constant-Œ±**: Q(s,a) ‚Üê Q(s,a) + Œ±[G - Q(s,a)]\n",
        "- **Advantage of Constant-Œ±**: Adapts to non-stationary environments, gives more weight to recent experience\n",
        "\n",
        "#### 2. **Why Q-values Work Better**\n",
        "- Direct policy improvement without environment model\n",
        "- œÄ(s) = argmax_a Q(s,a) is straightforward\n",
        "- No need for transition probabilities\n",
        "\n",
        "#### 3. **MDP ‚Üí MRP Transformation**\n",
        "When we fix policy œÄ:\n",
        "- **Original MDP**: Multiple actions per state\n",
        "- **Resulting MRP**: Single action per state (determined by œÄ)\n",
        "- **Benefit**: Can use MRP solution methods for evaluation\n",
        "\n",
        "#### 4. **Episode-based Learning**\n",
        "- Must wait for episode completion\n",
        "- Good for episodic tasks like Blackjack\n",
        "- Natural handling of terminal states\n",
        "\n",
        "#### 5. **Exploration vs Exploitation**\n",
        "- Œµ-greedy ensures all state-action pairs are visited\n",
        "- Critical for finding optimal policy\n",
        "- Balance between learning and performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def every_visit_monte_carlo_control(env, num_episodes=50000, alpha=0.01, epsilon=0.1, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Every-visit Monte Carlo Control (vs First-visit)\n",
        "    Updates Q-values for every occurrence of (s,a) in an episode\n",
        "    \"\"\"\n",
        "    Q = defaultdict(lambda: np.zeros(2))\n",
        "    \n",
        "    print(f\"Running Every-Visit Monte Carlo Control...\")\n",
        "    \n",
        "    for episode_num in tqdm(range(num_episodes)):\n",
        "        # Generate episode\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        \n",
        "        while True:\n",
        "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "            state = next_state\n",
        "        \n",
        "        # Update Q-values for every visit (not just first visit)\n",
        "        G = 0\n",
        "        \n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            \n",
        "            # Every-visit update (no check for previous visits)\n",
        "            Q[state][action] += alpha * (G - Q[state][action])\n",
        "    \n",
        "    return dict(Q)\n",
        "\n",
        "def off_policy_monte_carlo_control(env, num_episodes=50000, alpha=0.01, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Off-policy Monte Carlo Control using Importance Sampling\n",
        "    Target policy: greedy, Behavior policy: Œµ-greedy\n",
        "    \"\"\"\n",
        "    Q = defaultdict(lambda: np.zeros(2))\n",
        "    C = defaultdict(lambda: np.zeros(2))  # Cumulative weights\n",
        "    \n",
        "    epsilon = 0.1  # For behavior policy\n",
        "    \n",
        "    print(f\"Running Off-Policy Monte Carlo Control...\")\n",
        "    \n",
        "    for episode_num in tqdm(range(num_episodes)):\n",
        "        # Generate episode using behavior policy (Œµ-greedy)\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        \n",
        "        while True:\n",
        "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "            state = next_state\n",
        "        \n",
        "        # Importance sampling update\n",
        "        G = 0\n",
        "        W = 1  # Importance sampling ratio\n",
        "        \n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            \n",
        "            # Update with importance sampling\n",
        "            C[state][action] += W\n",
        "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "            \n",
        "            # Check if action matches target policy (greedy)\n",
        "            greedy_action = np.argmax(Q[state])\n",
        "            if action != greedy_action:\n",
        "                break  # Trajectory ends for target policy\n",
        "            \n",
        "            # Update importance sampling ratio\n",
        "            behavior_prob = epsilon/2 + (1-epsilon) * (action == greedy_action)\n",
        "            target_prob = 1.0  # Greedy policy probability\n",
        "            W *= target_prob / behavior_prob\n",
        "    \n",
        "    return dict(Q)\n",
        "\n",
        "print(\"Advanced Monte Carlo methods defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Compare different Monte Carlo variants\n",
        "print(\"Comparing Monte Carlo Variants...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Run every-visit Monte Carlo\n",
        "print(\"\\n1. Every-Visit Monte Carlo:\")\n",
        "Q_every_visit = every_visit_monte_carlo_control(env, num_episodes=30000)\n",
        "every_visit_policy = extract_policy(Q_every_visit)\n",
        "\n",
        "def every_visit_policy_func(state):\n",
        "    return policy_function(state, every_visit_policy)\n",
        "\n",
        "every_visit_results = test_policy(env, every_visit_policy_func, 5000)\n",
        "print(f\"Average Reward: {every_visit_results['avg_reward']:.4f}\")\n",
        "print(f\"Win Rate: {every_visit_results['win_rate']:.4f}\")\n",
        "\n",
        "# Run off-policy Monte Carlo\n",
        "print(\"\\n2. Off-Policy Monte Carlo:\")\n",
        "Q_off_policy = off_policy_monte_carlo_control(env, num_episodes=30000)\n",
        "off_policy_policy = extract_policy(Q_off_policy)\n",
        "\n",
        "def off_policy_policy_func(state):\n",
        "    return policy_function(state, off_policy_policy)\n",
        "\n",
        "off_policy_results = test_policy(env, off_policy_policy_func, 5000)\n",
        "print(f\"Average Reward: {off_policy_results['avg_reward']:.4f}\")\n",
        "print(f\"Win Rate: {off_policy_results['win_rate']:.4f}\")\n",
        "\n",
        "# Compare all methods\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON OF ALL METHODS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Method':<25} {'Avg Reward':<12} {'Win Rate':<10}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Simple Policy':<25} {simple_results['avg_reward']:<12.4f} {simple_results['win_rate']:<10.4f}\")\n",
        "print(f\"{'First-Visit MC':<25} {optimal_results['avg_reward']:<12.4f} {optimal_results['win_rate']:<10.4f}\")\n",
        "print(f\"{'Every-Visit MC':<25} {every_visit_results['avg_reward']:<12.4f} {every_visit_results['win_rate']:<10.4f}\")\n",
        "print(f\"{'Off-Policy MC':<25} {off_policy_results['avg_reward']:<12.4f} {off_policy_results['win_rate']:<10.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def step_by_step_monte_carlo_demo():\n",
        "    \"\"\"\n",
        "    Detailed step-by-step demonstration of Monte Carlo learning\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP-BY-STEP MONTE CARLO DEMONSTRATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Initialize\n",
        "    Q = defaultdict(lambda: np.zeros(2))\n",
        "    alpha = 0.1\n",
        "    epsilon = 0.1\n",
        "    \n",
        "    print(f\"\\nInitial Setup:\")\n",
        "    print(f\"Learning rate (Œ±): {alpha}\")\n",
        "    print(f\"Exploration rate (Œµ): {epsilon}\")\n",
        "    print(f\"Q-values initialized to 0\")\n",
        "    \n",
        "    # Demonstrate a few episodes\n",
        "    for episode_num in range(3):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EPISODE {episode_num + 1}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        # Generate episode\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        print(f\"\\nInitial state: {state}\")\n",
        "        \n",
        "        step = 0\n",
        "        while True:\n",
        "            # Choose action\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice([0, 1])\n",
        "                action_type = \"Random (exploration)\"\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "                action_type = \"Greedy (exploitation)\"\n",
        "            \n",
        "            print(f\"\\nStep {step + 1}:\")\n",
        "            print(f\"  State: {state}\")\n",
        "            print(f\"  Q(s,STICK): {Q[state][0]:.3f}, Q(s,HIT): {Q[state][1]:.3f}\")\n",
        "            print(f\"  Action: {'HIT' if action == 1 else 'STICK'} ({action_type})\")\n",
        "            \n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            \n",
        "            print(f\"  Reward: {reward}\")\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                print(f\"  Episode terminated\")\n",
        "                break\n",
        "            \n",
        "            state = next_state\n",
        "            step += 1\n",
        "        \n",
        "        # Update Q-values\n",
        "        print(f\"\\nUpdating Q-values:\")\n",
        "        G = 0\n",
        "        visited_pairs = set()\n",
        "        \n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = G + reward  # gamma = 1 for simplicity\n",
        "            \n",
        "            if (state, action) not in visited_pairs:\n",
        "                old_q = Q[state][action]\n",
        "                Q[state][action] += alpha * (G - Q[state][action])\n",
        "                new_q = Q[state][action]\n",
        "                \n",
        "                print(f\"  State {state}, Action {'HIT' if action == 1 else 'STICK'}:\")\n",
        "                print(f\"    Return G = {G}\")\n",
        "                print(f\"    Old Q = {old_q:.3f}\")\n",
        "                print(f\"    New Q = {old_q:.3f} + {alpha} * ({G} - {old_q:.3f}) = {new_q:.3f}\")\n",
        "                \n",
        "                visited_pairs.add((state, action))\n",
        "        \n",
        "        print(f\"\\nEpisode {episode_num + 1} complete!\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DEMONSTRATION COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"\\nKey observations:\")\n",
        "    print(\"1. Each episode provides complete return information\")\n",
        "    print(\"2. Q-values are updated using actual returns (not estimates)\")\n",
        "    print(\"3. Exploration ensures we visit different state-action pairs\")\n",
        "    print(\"4. Learning happens after each complete episode\")\n",
        "\n",
        "# Run the demonstration\n",
        "step_by_step_monte_carlo_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We've Accomplished:\n",
        "\n",
        "1. **Implemented Monte Carlo Evaluation**\n",
        "   - Estimated value functions from episode samples\n",
        "   - Used first-visit averaging method\n",
        "   - Demonstrated model-free learning\n",
        "\n",
        "2. **Implemented Monte Carlo Control**\n",
        "   - Found optimal policy through Q-value estimation\n",
        "   - Used Œµ-greedy exploration strategy\n",
        "   - Applied constant-Œ± updates for adaptability\n",
        "\n",
        "3. **Explored Advanced Techniques**\n",
        "   - Every-visit vs First-visit methods\n",
        "   - Off-policy learning with importance sampling\n",
        "   - Comparative analysis of different approaches\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "#### Why Q-values over V-values?\n",
        "- **Model-free policy improvement**: œÄ(s) = argmax_a Q(s,a)\n",
        "- **No environment dynamics needed**: Direct action selection\n",
        "- **Practical implementation**: Works in unknown environments\n",
        "\n",
        "#### MDP to MRP Transformation:\n",
        "- **Fixed policy creates MRP**: Single action per state\n",
        "- **Evaluation becomes simpler**: Standard MRP solution methods\n",
        "- **Theoretical foundation**: Justifies Monte Carlo evaluation\n",
        "\n",
        "#### Constant-Œ± Benefits:\n",
        "- **Adaptive learning**: Recent experience weighted more\n",
        "- **Non-stationary environments**: Handles changing dynamics\n",
        "- **Faster convergence**: In many practical scenarios\n",
        "\n",
        "### Monte Carlo Advantages:\n",
        "- ‚úÖ Model-free learning\n",
        "- ‚úÖ Unbiased value estimates\n",
        "- ‚úÖ Simple implementation\n",
        "- ‚úÖ Natural episode handling\n",
        "\n",
        "### Monte Carlo Limitations:\n",
        "- ‚ùå Requires complete episodes\n",
        "- ‚ùå High variance estimates\n",
        "- ‚ùå Slow convergence\n",
        "- ‚ùå Only for episodic tasks\n",
        "\n",
        "### Next Steps:\n",
        "- **Temporal Difference Learning**: Combines MC and DP benefits\n",
        "- **Function Approximation**: Handle large state spaces\n",
        "- **Policy Gradient Methods**: Direct policy optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def interactive_exercises():\n",
        "    \"\"\"\n",
        "    Interactive exercises for deeper understanding\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERACTIVE EXERCISES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\nExercise 1: Policy Comparison\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    def conservative_policy(state):\n",
        "        \"\"\"Very conservative policy - stick early\"\"\"\n",
        "        player_sum = state[0]\n",
        "        return 1 if player_sum < 17 else 0\n",
        "    \n",
        "    def aggressive_policy(state):\n",
        "        \"\"\"Aggressive policy - hit more often\"\"\"\n",
        "        player_sum = state[0]\n",
        "        return 1 if player_sum < 19 else 0\n",
        "    \n",
        "    # Test policies\n",
        "    conservative_results = test_policy(env, conservative_policy, 5000)\n",
        "    aggressive_results = test_policy(env, aggressive_policy, 5000)\n",
        "    \n",
        "    print(f\"Conservative Policy (stick at 17+): {conservative_results['avg_reward']:.4f}\")\n",
        "    print(f\"Aggressive Policy (stick at 19+):   {aggressive_results['avg_reward']:.4f}\")\n",
        "    print(f\"Optimal MC Policy:                   {optimal_results['avg_reward']:.4f}\")\n",
        "    \n",
        "    print(\"\\nExercise 2: Learning Rate Impact\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    alphas = [0.001, 0.01, 0.1, 0.5]\n",
        "    alpha_results = {}\n",
        "    \n",
        "    for alpha in alphas:\n",
        "        print(f\"Testing Œ± = {alpha}...\")\n",
        "        Q_alpha, _ = constant_alpha_monte_carlo_control(\n",
        "            env, num_episodes=20000, alpha=alpha, epsilon=0.1\n",
        "        )\n",
        "        policy_alpha = extract_policy(Q_alpha)\n",
        "        \n",
        "        def alpha_policy(state):\n",
        "            return policy_function(state, policy_alpha)\n",
        "        \n",
        "        results = test_policy(env, alpha_policy, 3000)\n",
        "        alpha_results[alpha] = results['avg_reward']\n",
        "    \n",
        "    print(\"\\nLearning Rate Results:\")\n",
        "    for alpha, reward in alpha_results.items():\n",
        "        print(f\"Œ± = {alpha:5.3f}: Average Reward = {reward:.4f}\")\n",
        "    \n",
        "    best_alpha = max(alpha_results, key=alpha_results.get)\n",
        "    print(f\"\\nBest learning rate: Œ± = {best_alpha}\")\n",
        "    \n",
        "    print(\"\\nExercise 3: Exploration Impact\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "    epsilon_results = {}\n",
        "    \n",
        "    for epsilon in epsilons:\n",
        "        print(f\"Testing Œµ = {epsilon}...\")\n",
        "        Q_epsilon, _ = constant_alpha_monte_carlo_control(\n",
        "            env, num_episodes=20000, alpha=0.01, epsilon=epsilon\n",
        "        )\n",
        "        policy_epsilon = extract_policy(Q_epsilon)\n",
        "        \n",
        "        def epsilon_policy(state):\n",
        "            return policy_function(state, policy_epsilon)\n",
        "        \n",
        "        results = test_policy(env, epsilon_policy, 3000)\n",
        "        epsilon_results[epsilon] = results['avg_reward']\n",
        "    \n",
        "    print(\"\\nExploration Rate Results:\")\n",
        "    for epsilon, reward in epsilon_results.items():\n",
        "        print(f\"Œµ = {epsilon:5.3f}: Average Reward = {reward:.4f}\")\n",
        "    \n",
        "    best_epsilon = max(epsilon_results, key=epsilon_results.get)\n",
        "    print(f\"\\nBest exploration rate: Œµ = {best_epsilon}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXERCISES COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nKey Observations:\")\n",
        "    print(\"1. Policy design significantly impacts performance\")\n",
        "    print(\"2. Learning rate affects convergence speed and stability\")\n",
        "    print(\"3. Exploration rate balances learning vs performance\")\n",
        "    print(\"4. Hyperparameter tuning is crucial for optimal results\")\n",
        "\n",
        "# Run interactive exercises\n",
        "interactive_exercises()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def monte_carlo_tree_search_preview():\n",
        "    \"\"\"\n",
        "    Preview of Monte Carlo Tree Search (MCTS) concepts\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BONUS: MONTE CARLO TREE SEARCH PREVIEW\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\nMonte Carlo Tree Search (MCTS) extends MC methods:\")\n",
        "    print(\"\\n1. **Selection**: Navigate tree using UCB1\")\n",
        "    print(\"2. **Expansion**: Add new nodes to tree\")\n",
        "    print(\"3. **Simulation**: Run random rollout\")\n",
        "    print(\"4. **Backpropagation**: Update all nodes in path\")\n",
        "    \n",
        "    print(\"\\nKey Differences from Standard MC:\")\n",
        "    print(\"- **Tree Structure**: Builds search tree incrementally\")\n",
        "    print(\"- **UCB1 Selection**: Balances exploration/exploitation\")\n",
        "    print(\"- **Partial Episodes**: Can work with incomplete information\")\n",
        "    print(\"- **Planning**: Looks ahead rather than just learning\")\n",
        "    \n",
        "    print(\"\\nApplications:\")\n",
        "    print(\"- Game AI (Go, Chess, Poker)\")\n",
        "    print(\"- Resource allocation\")\n",
        "    print(\"- Combinatorial optimization\")\n",
        "    print(\"- Real-time decision making\")\n",
        "    \n",
        "    print(\"\\nSimple MCTS-inspired action selection:\")\n",
        "    \n",
        "    def ucb1_action_selection(Q, N, state, c=1.414):\n",
        "        \"\"\"\n",
        "        UCB1 action selection (simplified)\n",
        "        \"\"\"\n",
        "        if state not in N or sum(N[state]) == 0:\n",
        "            return random.choice([0, 1])\n",
        "        \n",
        "        total_visits = sum(N[state])\n",
        "        ucb_values = []\n",
        "        \n",
        "        for a in range(2):\n",
        "            if N[state][a] == 0:\n",
        "                ucb_values.append(float('inf'))\n",
        "            else:\n",
        "                exploitation = Q[state][a]\n",
        "                exploration = c * np.sqrt(np.log(total_visits) / N[state][a])\n",
        "                ucb_values.append(exploitation + exploration)\n",
        "        \n",
        "        return np.argmax(ucb_values)\n",
        "    \n",
        "    # Demonstrate UCB1 vs Œµ-greedy\n",
        "    Q_demo = {(20, 10, False): np.array([0.5, -0.2])}\n",
        "    N_demo = {(20, 10, False): np.array([100, 10])}\n",
        "    \n",
        "    state_demo = (20, 10, False)\n",
        "    \n",
        "    print(f\"\\nExample state: {state_demo}\")\n",
        "    print(f\"Q-values: STICK={Q_demo[state_demo][0]:.2f}, HIT={Q_demo[state_demo][1]:.2f}\")\n",
        "    print(f\"Visit counts: STICK={N_demo[state_demo][0]}, HIT={N_demo[state_demo][1]}\")\n",
        "    \n",
        "    ucb1_action = ucb1_action_selection(Q_demo, N_demo, state_demo)\n",
        "    greedy_action = np.argmax(Q_demo[state_demo])\n",
        "    \n",
        "    print(f\"\\nGreedy choice: {'STICK' if greedy_action == 0 else 'HIT'}\")\n",
        "    print(f\"UCB1 choice: {'STICK' if ucb1_action == 0 else 'HIT'}\")\n",
        "    print(\"\\nUCB1 considers both value and uncertainty!\")\n",
        "\n",
        "# Run MCTS preview\n",
        "monte_carlo_tree_search_preview()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Final summary of learned policies\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY: MONTE CARLO METHODS IN BLACKJACK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüéØ MISSION ACCOMPLISHED:\")\n",
        "print(\"‚úÖ Implemented Monte Carlo Evaluation\")\n",
        "print(\"‚úÖ Implemented Monte Carlo Control\")\n",
        "print(\"‚úÖ Explained Q-value vs V-value estimation\")\n",
        "print(\"‚úÖ Demonstrated MDP to MRP transformation\")\n",
        "print(\"‚úÖ Applied constant-Œ± learning\")\n",
        "print(\"‚úÖ Found optimal policy without analysis\")\n",
        "print(\"‚úÖ Explored advanced MC techniques\")\n",
        "print(\"‚úÖ Conducted interactive experiments\")\n",
        "\n",
        "print(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
        "print(f\"Simple Policy Performance:     {simple_results['avg_reward']:.4f}\")\n",
        "print(f\"Monte Carlo Optimal Policy:    {optimal_results['avg_reward']:.4f}\")\n",
        "print(f\"Improvement:                   {optimal_results['avg_reward'] - simple_results['avg_reward']:.4f}\")\n",
        "print(f\"Win Rate Improvement:          {optimal_results['win_rate'] - simple_results['win_rate']:.4f}\")\n",
        "\n",
        "print(\"\\nüß† KEY LEARNINGS:\")\n",
        "print(\"1. Monte Carlo methods learn from complete episodes\")\n",
        "print(\"2. Q-values enable model-free policy improvement\")\n",
        "print(\"3. Constant-Œ± provides adaptive learning\")\n",
        "print(\"4. Exploration is crucial for finding optimal policies\")\n",
        "print(\"5. Different MC variants have different trade-offs\")\n",
        "\n",
        "print(\"\\nüöÄ NEXT STEPS:\")\n",
        "print(\"- Temporal Difference Learning (TD)\")\n",
        "print(\"- Q-Learning and SARSA\")\n",
        "print(\"- Deep Q-Networks (DQN)\")\n",
        "print(\"- Policy Gradient Methods\")\n",
        "print(\"- Actor-Critic Methods\")\n",
        "\n",
        "# Save final policy for future use\n",
        "import pickle\n",
        "\n",
        "final_results = {\n",
        "    'optimal_policy': optimal_policy_dict,\n",
        "    'Q_values': Q_optimal,\n",
        "    'performance': optimal_results,\n",
        "    'training_rewards': rewards\n",
        "}\n",
        "\n",
        "# Uncomment to save results\n",
        "# with open('monte_carlo_blackjack_results.pkl', 'wb') as f:\n",
        "#     pickle.dump(final_results, f)\n",
        "# print(\"\\nüíæ Results saved to 'monte_carlo_blackjack_results.pkl'\")\n",
        "\n",
        "print(\"\\nüéâ MONTE CARLO LEARNING COMPLETE!\")\n",
        "print(\"Thank you for exploring Monte Carlo methods in Reinforcement Learning!\")\n",
        "\n",
        "# Close environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources and References\n",
        "\n",
        "### üìö Recommended Reading:\n",
        "\n",
        "1. **Sutton & Barto (2018)**: \"Reinforcement Learning: An Introduction\" - Chapter 5\n",
        "2. **Bertsekas (2019)**: \"Reinforcement Learning and Optimal Control\"\n",
        "3. **Szepesv√°ri (2010)**: \"Algorithms for Reinforcement Learning\"\n",
        "\n",
        "### üîó Online Resources:\n",
        "\n",
        "- [OpenAI Gymnasium Documentation](https://gymnasium.farama.org/)\n",
        "- [Reinforcement Learning Course by David Silver](https://www.davidsilver.uk/teaching/)\n",
        "- [CS285 Deep RL Course (UC Berkeley)](http://rail.eecs.berkeley.edu/deeprlcourse/)\n",
        "\n",
        "### üõ†Ô∏è Implementation Notes:\n",
        "\n",
        "#### Monte Carlo Evaluation:\n",
        "```python\n",
        "# Pseudocode\n",
        "for episode in episodes:\n",
        "    G = 0\n",
        "    for t in reversed(episode):\n",
        "        G = Œ≥ * G + R[t+1]\n",
        "        if S[t] not in visited:\n",
        "            Returns[S[t]].append(G)\n",
        "            V[S[t]] = average(Returns[S[t]])\n",
        "```\n",
        "\n",
        "#### Monte Carlo Control:\n",
        "```python\n",
        "# Pseudocode\n",
        "for episode in episodes:\n",
        "    # Generate episode using Œµ-greedy policy\n",
        "    G = 0\n",
        "    for t in reversed(episode):\n",
        "        G = Œ≥ * G + R[t+1]\n",
        "        if (S[t], A[t]) not in visited:\n",
        "            Q[S[t]][A[t]] += Œ± * (G - Q[S[t]][A[t]])\n",
        "            œÄ[S[t]] = argmax_a Q[S[t]][a]\n",
        "```\n",
        "\n",
        "### üéØ Exercise Solutions:\n",
        "\n",
        "Try implementing these variations:\n",
        "\n",
        "1. **Weighted Importance Sampling**: For off-policy learning\n",
        "2. **Incremental Implementation**: Update estimates incrementally\n",
        "3. **Discounting**: Experiment with Œ≥ < 1\n",
        "4. **Different Exploration**: Try UCB or Boltzmann exploration\n",
        "\n",
        "### üêõ Common Pitfalls:\n",
        "\n",
        "- **Insufficient Exploration**: Policy may converge to suboptimal solution\n",
        "- **High Learning Rate**: Can cause instability\n",
        "- **Low Learning Rate**: Slow convergence\n",
        "- **Forgetting First-Visit**: Every-visit can introduce bias\n",
        "\n",
        "### üî¨ Extensions to Explore:\n",
        "\n",
        "1. **Monte Carlo Tree Search (MCTS)**\n",
        "2. **Gradient Monte Carlo**\n",
        "3. **Natural Policy Gradients**\n",
        "4. **Monte Carlo with Function Approximation**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}