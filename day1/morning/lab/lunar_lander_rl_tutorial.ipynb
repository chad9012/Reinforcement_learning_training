{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Interactive Reinforcement Learning Tutorial\n",
        "## Understanding the Basics with Gymnasium\n",
        "\n",
        "Welcome to this interactive tutorial on Reinforcement Learning! We'll explore the fundamental concepts using practical examples with OpenAI's Gymnasium library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. What is Reinforcement Learning?\n",
        "\n",
        "**Reinforcement Learning (RL)** is a learning paradigm where an **agent** learns from interaction with an **environment** to maximize cumulative reward over time.\n",
        "\n",
        "### Key Characteristics:\n",
        "- ğŸ¯ **Goal-oriented**: Agent learns to achieve objectives\n",
        "- ğŸ”„ **Trial and error**: Learning through experience\n",
        "- â° **Sequential decision making**: Actions affect future states\n",
        "- ğŸ† **Reward-driven**: Behavior shaped by feedback\n",
        "\n",
        "### RL vs Other Learning Paradigms:\n",
        "- **Supervised Learning**: Learn from labeled examples\n",
        "- **Unsupervised Learning**: Find patterns in data\n",
        "- **Reinforcement Learning**: Learn from interaction and feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. The Markov Decision Process (MDP) Framework\n",
        "\n",
        "RL problems are modeled as **Markov Decision Processes (MDPs)** with five key components:\n",
        "\n",
        "### ğŸ—ï¸ MDP Components:\n",
        "1. **S**: Set of **States** - All possible environment configurations\n",
        "2. **A**: Set of **Actions** - All possible agent actions  \n",
        "3. **P(s'|s,a)**: **Transition Probability** - Probability of reaching state s' from state s after action a\n",
        "4. **R(s,a)**: **Reward Function** - Immediate reward for taking action a in state s\n",
        "5. **Î³ (gamma)**: **Discount Factor** - Importance of future rewards (0 â‰¤ Î³ â‰¤ 1)\n",
        "\n",
        "### ğŸ¯ Objective:\n",
        "Learn a **policy Ï€(a|s)** that maximizes expected cumulative reward (return).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Reinforcement Learning with LunarLander\n",
        "\n",
        "## ğŸ“š Table of Contents\n",
        "\n",
        "Welcome to hands-on reinforcement learning! In this tutorial, we'll:\n",
        "\n",
        "1. ğŸ“¦ Set up the required packages\n",
        "2. ğŸŒ™ Explore the LunarLander environment\n",
        "3. ğŸ² Test a random agent baseline\n",
        "4. ğŸ® Take manual control of the lander\n",
        "\n",
        "### ğŸ¯ Learning Objectives:\n",
        "- Understand MDP components in practice\n",
        "- Analyze state and action spaces\n",
        "- Experience the challenge of RL environments\n",
        "- Compare random vs intelligent control\n",
        "\n",
        "Let's begin our lunar landing mission! ğŸŒ™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… gymnasium already installed\n",
            "ğŸ“¦ Installing gymnasium[box2d]...\n",
            "Requirement already satisfied: gymnasium[box2d] in /home/pc6/anaconda3/envs/rl_env/lib/python3.10/site-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/pc6/anaconda3/envs/rl_env/lib/python3.10/site-packages (from gymnasium[box2d]) (2.2.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/pc6/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/pc6/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/pc6/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /home/pc6/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /home/pc6/anaconda3/envs/rl_env/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /home/pc6/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (4.3.1)\n",
            "âœ… gymnasium[box2d] installed successfully\n",
            "âœ… numpy already installed\n",
            "âœ… matplotlib already installed\n",
            "âœ… pygame already installed\n",
            "\n",
            "ğŸš€ All packages ready for lunar landing!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this first!)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"âœ… {package} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"ğŸ“¦ Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ… {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_package(\"gymnasium\")\n",
        "install_package(\"gymnasium[box2d]\")  # For LunarLander\n",
        "install_package(\"numpy\")\n",
        "install_package(\"matplotlib\")\n",
        "install_package(\"pygame\")\n",
        "\n",
        "print(\"\\nğŸš€ All packages ready for lunar landing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“š Libraries imported successfully!\n",
            "ğŸ‹ï¸ Gymnasium version: 1.1.1\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pygame\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(\"ğŸ“š Libraries imported successfully!\")\n",
        "print(f\"ğŸ‹ï¸ Gymnasium version: {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ LunarLander-v2 Environment Analysis\n",
            "==================================================\n",
            "ğŸ—ï¸  State Space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "    - Type: <class 'gymnasium.spaces.box.Box'>\n",
            "    - Shape: (8,)\n",
            "    - Low bounds: [ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ]\n",
            "    - High bounds: [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ]\n",
            "\n",
            "ğŸ¯ Action Space: Discrete(4)\n",
            "    - Type: <class 'gymnasium.spaces.discrete.Discrete'>\n",
            "    - Number of actions: 4\n",
            "    - Actions:\n",
            "      â€¢ 0: Do nothing\n",
            "      â€¢ 1: Fire left orientation engine\n",
            "      â€¢ 2: Fire main engine\n",
            "      â€¢ 3: Fire right orientation engine\n",
            "\n",
            "ğŸ† Reward Range: (-âˆ, +âˆ) - Continuous reward system\n",
            "ğŸ“Š Max Episode Steps: 1000\n",
            "\n",
            "ğŸ” State Vector Details (8 dimensions):\n",
            "    0: Horizontal position (x)\n",
            "    1: Vertical position (y)\n",
            "    2: Horizontal velocity (vx)\n",
            "    3: Vertical velocity (vy)\n",
            "    4: Angle (Î¸)\n",
            "    5: Angular velocity (Ï‰)\n",
            "    6: Left leg contact (boolean)\n",
            "    7: Right leg contact (boolean)\n",
            "\n",
            "ğŸ¯ Mission Objective:\n",
            "    â€¢ Land the lunar module safely between the flags\n",
            "    â€¢ Reward â‰¥ 200 points = Successful landing\n",
            "    â€¢ Reward â‰¥ 100 points = Decent attempt\n",
            "    â€¢ Negative rewards = Crashed or poor landing\n"
          ]
        }
      ],
      "source": [
        "## ğŸŒ™ Exploring the LunarLander Environment\n",
        "\n",
        "# Create and explore the LunarLander environment\n",
        "env = gym.make('LunarLander-v3')\n",
        "\n",
        "print(\"ğŸš€ LunarLander-v2 Environment Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Environment spaces\n",
        "print(f\"ğŸ—ï¸  State Space: {env.observation_space}\")\n",
        "print(f\"    - Type: {type(env.observation_space)}\")\n",
        "print(f\"    - Shape: {env.observation_space.shape}\")\n",
        "print(f\"    - Low bounds: {env.observation_space.low}\")\n",
        "print(f\"    - High bounds: {env.observation_space.high}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Action Space: {env.action_space}\")\n",
        "print(f\"    - Type: {type(env.action_space)}\")\n",
        "print(f\"    - Number of actions: {env.action_space.n}\")\n",
        "print(f\"    - Actions:\")\n",
        "print(f\"      â€¢ 0: Do nothing\")\n",
        "print(f\"      â€¢ 1: Fire left orientation engine\")\n",
        "print(f\"      â€¢ 2: Fire main engine\")\n",
        "print(f\"      â€¢ 3: Fire right orientation engine\")\n",
        "\n",
        "# Handle reward range safely\n",
        "try:\n",
        "    print(f\"\\nğŸ† Reward Range: {env.reward_range}\")\n",
        "except AttributeError:\n",
        "    print(f\"\\nğŸ† Reward Range: (-âˆ, +âˆ) - Continuous reward system\")\n",
        "\n",
        "print(f\"ğŸ“Š Max Episode Steps: {env.spec.max_episode_steps}\")\n",
        "\n",
        "# Show state space details\n",
        "print(f\"\\nğŸ” State Vector Details (8 dimensions):\")\n",
        "state_descriptions = [\n",
        "    \"0: Horizontal position (x)\",\n",
        "    \"1: Vertical position (y)\", \n",
        "    \"2: Horizontal velocity (vx)\",\n",
        "    \"3: Vertical velocity (vy)\",\n",
        "    \"4: Angle (Î¸)\",\n",
        "    \"5: Angular velocity (Ï‰)\",\n",
        "    \"6: Left leg contact (boolean)\",\n",
        "    \"7: Right leg contact (boolean)\"\n",
        "]\n",
        "\n",
        "for desc in state_descriptions:\n",
        "    print(f\"    {desc}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Mission Objective:\")\n",
        "print(f\"    â€¢ Land the lunar module safely between the flags\")\n",
        "print(f\"    â€¢ Reward â‰¥ 200 points = Successful landing\")\n",
        "print(f\"    â€¢ Reward â‰¥ 100 points = Decent attempt\")\n",
        "print(f\"    â€¢ Negative rewards = Crashed or poor landing\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Sample Initial State:\n",
            "==============================\n",
            "State[0]:   0.0023 - Horizontal position (x)\n",
            "State[1]:   1.4181 - Vertical position (y)\n",
            "State[2]:   0.2326 - Horizontal velocity (vx)\n",
            "State[3]:   0.3205 - Vertical velocity (vy)\n",
            "State[4]:  -0.0027 - Angle (Î¸)\n",
            "State[5]:  -0.0527 - Angular velocity (Ï‰)\n",
            "State[6]:   0.0000 - Left leg contact\n",
            "State[7]:   0.0000 - Right leg contact\n",
            "\n",
            "ğŸ’¡ Interpretation:\n",
            "   â€¢ Lander starts at position (0.002, 1.418)\n",
            "   â€¢ Initial velocity: (0.233, 0.320)\n",
            "   â€¢ Angle: -0.003 radians (-0.2Â°)\n",
            "   â€¢ Legs touching ground: False (left), False (right)\n"
          ]
        }
      ],
      "source": [
        "## ğŸ“Š Sample State Observation\n",
        "\n",
        "# Let's see what a typical state looks like\n",
        "env = gym.make('LunarLander-v3')\n",
        "state, info = env.reset(seed=42)  # Set seed for reproducibility\n",
        "\n",
        "print(\"ğŸ” Sample Initial State:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "state_labels = [\n",
        "    \"Horizontal position (x)\",\n",
        "    \"Vertical position (y)\", \n",
        "    \"Horizontal velocity (vx)\",\n",
        "    \"Vertical velocity (vy)\",\n",
        "    \"Angle (Î¸)\",\n",
        "    \"Angular velocity (Ï‰)\",\n",
        "    \"Left leg contact\",\n",
        "    \"Right leg contact\"\n",
        "]\n",
        "\n",
        "for i, (value, label) in enumerate(zip(state, state_labels)):\n",
        "    print(f\"State[{i}]: {value:8.4f} - {label}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Interpretation:\")\n",
        "print(f\"   â€¢ Lander starts at position ({state[0]:.3f}, {state[1]:.3f})\")\n",
        "print(f\"   â€¢ Initial velocity: ({state[2]:.3f}, {state[3]:.3f})\")\n",
        "print(f\"   â€¢ Angle: {state[4]:.3f} radians ({np.degrees(state[4]):.1f}Â°)\")\n",
        "print(f\"   â€¢ Legs touching ground: {bool(state[6])} (left), {bool(state[7])} (right)\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Random Agent class created successfully!\n",
            "ğŸ“Š Episode runner function ready with human-readable output!\n",
            "ğŸ¯ Demo function ready - use run_random_agent_demo(env) to see it in action!\n"
          ]
        }
      ],
      "source": [
        "## ğŸ² Random Agent Implementation\n",
        "\n",
        "class RandomAgent:\n",
        "    \"\"\"Random agent that selects actions randomly\"\"\"\n",
        "    \n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select a random action\"\"\"\n",
        "        return self.action_space.sample()\n",
        "    \n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Random agents don't learn\"\"\"\n",
        "        pass\n",
        "\n",
        "def run_episode(env, agent, max_steps=1000, render=False, verbose=False):\n",
        "    \"\"\"Run a single episode and return episode data\"\"\"\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    action_counts = {0: 0, 1: 0, 2: 0, 3: 0}  # Track action usage\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.02)\n",
        "            \n",
        "        action = agent.select_action(state)\n",
        "        action_counts[action] += 1\n",
        "        \n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        if verbose and step % 100 == 0:\n",
        "            action_name = {0: \"DO NOTHING\", 1: \"FIRE LEFT\", 2: \"FIRE MAIN\", 3: \"FIRE RIGHT\"}[action]\n",
        "            print(f\"  Step {step}: Action={action_name}, Reward={reward:.2f}, Total={total_reward:.1f}\")\n",
        "        \n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "        \n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # Enhanced return with human-readable action summary\n",
        "    action_names = {0: \"DO NOTHING\", 1: \"FIRE LEFT\", 2: \"FIRE MAIN\", 3: \"FIRE RIGHT\"}\n",
        "    action_summary = {action_names[k]: v for k, v in action_counts.items()}\n",
        "    \n",
        "    return {\n",
        "        'total_reward': total_reward,\n",
        "        'steps': steps,\n",
        "        'success': total_reward >= 200,\n",
        "        'decent': total_reward >= 100,\n",
        "        'action_counts': action_counts,\n",
        "        'action_summary': action_summary,\n",
        "        'terminated': terminated if 'terminated' in locals() else False\n",
        "    }\n",
        "\n",
        "def print_episode_summary(episode_data):\n",
        "    \"\"\"Print a human-readable summary of the episode\"\"\"\n",
        "    print(f\"\\nğŸš€ Episode Summary:\")\n",
        "    print(f\"   Total Reward: {episode_data['total_reward']:.1f}\")\n",
        "    print(f\"   Steps Taken: {episode_data['steps']}\")\n",
        "    print(f\"   Success: {'âœ… YES' if episode_data['success'] else 'âŒ NO'}\")\n",
        "    print(f\"   Decent Landing: {'âœ… YES' if episode_data['decent'] else 'âŒ NO'}\")\n",
        "    \n",
        "    print(f\"\\nğŸ® Action Usage:\")\n",
        "    for action_name, count in episode_data['action_summary'].items():\n",
        "        percentage = (count / episode_data['steps']) * 100\n",
        "        print(f\"   {action_name}: {count} times ({percentage:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ Performance: {'ğŸ† EXCELLENT' if episode_data['success'] else 'ğŸ‘ GOOD' if episode_data['decent'] else 'ğŸ’¥ CRASHED'}\")\n",
        "\n",
        "def run_random_agent_demo(env, num_episodes=5, render=True, verbose=True):\n",
        "    \"\"\"Run a demonstration of the random agent with human-readable output\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ² RANDOM AGENT DEMONSTRATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"Watch the random agent attempt to land the lunar lander!\")\n",
        "    print(f\"Running {num_episodes} episodes...\")\n",
        "    \n",
        "    agent = RandomAgent(env.action_space)\n",
        "    all_results = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nğŸŒ™ Episode {episode + 1}/{num_episodes}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        episode_data = run_episode(env, agent, render=render, verbose=verbose)\n",
        "        all_results.append(episode_data)\n",
        "        \n",
        "        # Print summary for each episode\n",
        "        print_episode_summary(episode_data)\n",
        "        \n",
        "        if render and episode < num_episodes - 1:\n",
        "            input(\"\\nPress Enter to continue to next episode...\")\n",
        "    \n",
        "    # Overall statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ“Š OVERALL RANDOM AGENT STATISTICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    total_episodes = len(all_results)\n",
        "    successful_episodes = sum(1 for r in all_results if r['success'])\n",
        "    decent_episodes = sum(1 for r in all_results if r['decent'])\n",
        "    avg_reward = sum(r['total_reward'] for r in all_results) / total_episodes\n",
        "    avg_steps = sum(r['steps'] for r in all_results) / total_episodes\n",
        "    \n",
        "    print(f\"Total Episodes: {total_episodes}\")\n",
        "    print(f\"Successful Landings (â‰¥200 reward): {successful_episodes} ({successful_episodes/total_episodes*100:.1f}%)\")\n",
        "    print(f\"Decent Landings (â‰¥100 reward): {decent_episodes} ({decent_episodes/total_episodes*100:.1f}%)\")\n",
        "    print(f\"Average Reward: {avg_reward:.1f}\")\n",
        "    print(f\"Average Steps: {avg_steps:.1f}\")\n",
        "    \n",
        "    # Action distribution across all episodes\n",
        "    total_actions = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "    for result in all_results:\n",
        "        for action, count in result['action_counts'].items():\n",
        "            total_actions[action] += count\n",
        "    \n",
        "    total_action_count = sum(total_actions.values())\n",
        "    action_names = {0: \"DO NOTHING\", 1: \"FIRE LEFT\", 2: \"FIRE MAIN\", 3: \"FIRE RIGHT\"}\n",
        "    \n",
        "    print(f\"\\nğŸ® Overall Action Distribution:\")\n",
        "    for action_id, count in total_actions.items():\n",
        "        percentage = (count / total_action_count) * 100\n",
        "        print(f\"   {action_names[action_id]}: {count} times ({percentage:.1f}%)\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "print(\"ğŸ¤– Random Agent class created successfully!\")\n",
        "print(\"ğŸ“Š Episode runner function ready with human-readable output!\")\n",
        "print(\"ğŸ¯ Demo function ready - use run_random_agent_demo(env) to see it in action!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Testing Random Agent on LunarLander\n",
            "==================================================\n",
            "ğŸ® Running 5 episodes with random actions...\n",
            "\n",
            "ğŸš€ Episode 1:\n",
            "   Steps: 85, Reward: -402.1 - âŒ Crashed/Failed\n",
            "   Actions used: Do nothing=17, Left=17, Main=22, Right=29\n",
            "\n",
            "ğŸš€ Episode 2:\n",
            "   Steps: 110, Reward: -114.0 - âŒ Crashed/Failed\n",
            "   Actions used: Do nothing=30, Left=31, Main=19, Right=30\n",
            "\n",
            "ğŸš€ Episode 3:\n",
            "   Steps: 72, Reward: -107.5 - âŒ Crashed/Failed\n",
            "   Actions used: Do nothing=20, Left=20, Main=16, Right=16\n",
            "\n",
            "ğŸš€ Episode 4:\n",
            "   Steps: 99, Reward: -337.1 - âŒ Crashed/Failed\n",
            "   Actions used: Do nothing=27, Left=10, Main=32, Right=30\n",
            "\n",
            "ğŸš€ Episode 5:\n",
            "   Steps: 69, Reward: -85.6 - âŒ Crashed/Failed\n",
            "   Actions used: Do nothing=18, Left=21, Main=11, Right=19\n",
            "\n",
            "ğŸ“Š Random Agent Results Summary:\n",
            "===================================\n",
            "ğŸ¯ Success Rate: 0/5 (0%)\n",
            "ğŸŸ¡ Decent Attempts: 0/5 (0%)\n",
            "ğŸ“ˆ Average Reward: -209.3 Â± 132.9\n",
            "ğŸ† Best Episode: -85.6\n",
            "ğŸ’¥ Worst Episode: -402.1\n",
            "\n",
            "ğŸ® Action Distribution Across All Episodes:\n",
            "   Do nothing: 112 (25.7%)\n",
            "   Fire left: 99 (22.8%)\n",
            "   Fire main: 100 (23.0%)\n",
            "   Fire right: 124 (28.5%)\n",
            "\n",
            "ğŸ’¡ Observations:\n",
            "   â€¢ Random actions rarely lead to successful landings\n",
            "   â€¢ Most episodes end in crashes or poor performance\n",
            "   â€¢ This shows the need for intelligent control strategies\n",
            "   â€¢ Success rate: 0% (vs ~90%+ for trained agents)\n"
          ]
        }
      ],
      "source": [
        "## ğŸ² Testing Random Agent - 5 Episodes\n",
        "\n",
        "print(\"ğŸš€ Testing Random Agent on LunarLander\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create environment and random agent\n",
        "env = gym.make('LunarLander-v3',render_mode='human')\n",
        "random_agent = RandomAgent(env.action_space)\n",
        "\n",
        "# Test random agent for 5 episodes\n",
        "random_rewards = []\n",
        "successes = 0\n",
        "decent_attempts = 0\n",
        "all_action_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "\n",
        "print(\"ğŸ® Running 5 episodes with random actions...\\n\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"ğŸš€ Episode {i+1}:\")\n",
        "    episode_data = run_episode(env, random_agent, max_steps=1000, verbose=False)\n",
        "    \n",
        "    random_rewards.append(episode_data['total_reward'])\n",
        "    \n",
        "    # Update counters\n",
        "    if episode_data['success']:\n",
        "        successes += 1\n",
        "        status = \"âœ… SUCCESS!\"\n",
        "    elif episode_data['decent']:\n",
        "        decent_attempts += 1\n",
        "        status = \"ğŸŸ¡ Decent attempt\"\n",
        "    else:\n",
        "        status = \"âŒ Crashed/Failed\"\n",
        "    \n",
        "    # Aggregate action counts\n",
        "    for action, count in episode_data['action_counts'].items():\n",
        "        all_action_counts[action] += count\n",
        "    \n",
        "    print(f\"   Steps: {episode_data['steps']}, Reward: {episode_data['total_reward']:.1f} - {status}\")\n",
        "    print(f\"   Actions used: Do nothing={episode_data['action_counts'][0]}, Left={episode_data['action_counts'][1]}, Main={episode_data['action_counts'][2]}, Right={episode_data['action_counts'][3]}\")\n",
        "    print()\n",
        "\n",
        "# Calculate statistics\n",
        "avg_reward = np.mean(random_rewards)\n",
        "std_reward = np.std(random_rewards)\n",
        "best_reward = max(random_rewards)\n",
        "worst_reward = min(random_rewards)\n",
        "\n",
        "print(\"ğŸ“Š Random Agent Results Summary:\")\n",
        "print(\"=\" * 35)\n",
        "print(f\"ğŸ¯ Success Rate: {successes}/5 ({successes*20}%)\")\n",
        "print(f\"ğŸŸ¡ Decent Attempts: {decent_attempts}/5 ({decent_attempts*20}%)\")\n",
        "print(f\"ğŸ“ˆ Average Reward: {avg_reward:.1f} Â± {std_reward:.1f}\")\n",
        "print(f\"ğŸ† Best Episode: {best_reward:.1f}\")\n",
        "print(f\"ğŸ’¥ Worst Episode: {worst_reward:.1f}\")\n",
        "\n",
        "print(f\"\\nğŸ® Action Distribution Across All Episodes:\")\n",
        "total_actions = sum(all_action_counts.values())\n",
        "for action, count in all_action_counts.items():\n",
        "    action_names = ['Do nothing', 'Fire left', 'Fire main', 'Fire right']\n",
        "    percentage = (count / total_actions) * 100\n",
        "    print(f\"   {action_names[action]}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Observations:\")\n",
        "print(f\"   â€¢ Random actions rarely lead to successful landings\")\n",
        "print(f\"   â€¢ Most episodes end in crashes or poor performance\")\n",
        "print(f\"   â€¢ This shows the need for intelligent control strategies\")\n",
        "print(f\"   â€¢ Success rate: {successes*20}% (vs ~90%+ for trained agents)\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ LunarLander Manual Control Started!\n",
            "Goal: Land the spacecraft safely between the flags!\n",
            "Controls:\n",
            "  â† Left Arrow  = Fire left engine (rotate right)\n",
            "  â†’ Right Arrow = Fire right engine (rotate left)\n",
            "  â†‘ Up Arrow    = Fire main engine (thrust up)\n",
            "  â†“ Down Arrow  = Do nothing\n",
            "  ESC or Close Window = Quit\n",
            "  R = Reset episode\n",
            "==================================================\n",
            "ğŸ’¡ Tips:\n",
            "  - Use main engine to slow descent\n",
            "  - Use side engines to control rotation and horizontal movement\n",
            "  - Land gently between the yellow flags!\n",
            "  - Legs must touch ground first for safe landing\n",
            "==================================================\n",
            "Step  20 | X:  -0.04 | Y:   1.29 | Vel: (-0.19,-0.52) | Angle:  0.04 | Reward:   -1.6 | Total:  -28.7\n",
            "Step  40 | X:  -0.08 | Y:   0.93 | Vel: (-0.19,-1.05) | Angle:  0.09 | Reward:   -0.5 | Total:  -49.0\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step  60 | X:  -0.13 | Y:   0.49 | Vel: (-0.34,-0.82) | Angle:  0.11 | Reward:    2.6 | Total:    5.0\n",
            "Step  80 | X:  -0.21 | Y:   0.21 | Vel: (-0.52,-0.41) | Angle:  0.08 | Reward:    2.4 | Total:   45.0\n",
            "Step 100 | X:  -0.33 | Y:   0.13 | Vel: (-0.62, 0.01) | Angle:  0.02 | Reward:   -0.6 | Total:   42.6\n",
            "Step 120 | X:  -0.45 | Y:   0.25 | Vel: (-0.57, 0.51) | Angle: -0.01 | Reward:   -2.8 | Total:    8.4\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "â­• Engines off\n",
            "Step 140 | X:  -0.57 | Y:   0.50 | Vel: (-0.59, 0.37) | Angle: -0.07 | Reward:   -0.0 | Total:  -18.7\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step 160 | X:  -0.68 | Y:   0.54 | Vel: (-0.38,-0.07) | Angle: -0.47 | Reward:    1.4 | Total:  -38.4\n",
            "Step 173 | X:  -0.69 | Y:   0.55 | Vel: ( 0.18, 0.11) | Angle: -0.91 | Reward:  -11.1 | Total:  -71.3\n",
            "Step 175 | X:  -0.69 | Y:   0.56 | Vel: ( 0.32, 0.10) | Angle: -0.98 | Reward:  -11.4 | Total:  -90.8\n",
            "ğŸ”¥ LEFT ENGINE FIRING!\n",
            "Step 180 | X:  -0.67 | Y:   0.56 | Vel: ( 0.30,-0.01) | Angle: -1.12 | Reward:   -1.8 | Total: -101.0\n",
            "Step 200 | X:  -0.62 | Y:   0.46 | Vel: ( 0.22,-0.42) | Angle: -1.08 | Reward:    1.8 | Total: -104.6\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step 220 | X:  -0.46 | Y:   0.26 | Vel: ( 1.24,-0.37) | Angle: -0.54 | Reward:   -1.1 | Total: -114.9\n",
            "â­• Engines off\n",
            "Step 240 | X:  -0.16 | Y:   0.13 | Vel: ( 1.57,-0.41) | Angle:  0.05 | Reward:   -1.8 | Total:  -68.4\n",
            "ğŸ¦µ Landing legs touching: RIGHT\n",
            "Step 253 | X:   0.05 | Y:  -0.03 | Vel: ( 1.52,-0.42) | Angle:  0.16 | Reward: -100.0 | Total: -172.2\n",
            "\n",
            "==================================================\n",
            "ğŸ’¥ CRASH! The lander was destroyed.\n",
            "ğŸ“Š Episode 1 Results:\n",
            "   Final Score: -172.2\n",
            "   Steps taken: 253\n",
            "   Final position: (0.05, -0.03)\n",
            "   Final velocity: (1.52, -0.42)\n",
            "ğŸ’€ RATING: NEEDS MORE PRACTICE\n",
            "==================================================\n",
            "ğŸ”„ Resetting in 3 seconds... (Press R to reset immediately)\n",
            "ğŸ†• Episode 2 started\n",
            "Step  20 | X:  -0.04 | Y:   1.38 | Vel: (-0.19,-0.34) | Angle:  0.05 | Reward:   -1.8 | Total:  -12.1\n",
            "Step  40 | X:  -0.08 | Y:   1.10 | Vel: (-0.19,-0.87) | Angle:  0.09 | Reward:   -0.9 | Total:  -39.2\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step  60 | X:  -0.12 | Y:   0.69 | Vel: (-0.28,-0.72) | Angle:  0.14 | Reward:    3.4 | Total:    3.8\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "Step  80 | X:  -0.19 | Y:   0.42 | Vel: (-0.30,-0.70) | Angle:  0.20 | Reward:    0.1 | Total:   19.3\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step 100 | X:  -0.23 | Y:   0.08 | Vel: (-0.14,-0.57) | Angle: -0.22 | Reward:   -0.6 | Total:   51.8\n",
            "Step 108 | X:  -0.24 | Y:  -0.00 | Vel: (-0.02,-0.31) | Angle: -0.29 | Reward:   10.3 | Total:   80.4\n",
            "Step 110 | X:  -0.24 | Y:  -0.01 | Vel: (-0.01,-0.23) | Angle: -0.17 | Reward:   10.7 | Total:   99.2\n",
            "ğŸ¦µ Landing legs touching: LEFT\n",
            "Step 111 | X:  -0.24 | Y:  -0.02 | Vel: (-0.01,-0.19) | Angle: -0.11 | Reward:   10.2 | Total:  109.4\n",
            "Step 120 | X:  -0.23 | Y:  -0.03 | Vel: (-0.03, 0.01) | Angle:  0.44 | Reward:   -5.9 | Total:   90.5\n",
            "ğŸ¦µ Landing legs touching: RIGHT\n",
            "Step 121 | X:  -0.23 | Y:  -0.03 | Vel: (-0.10, 0.07) | Angle:  0.46 | Reward:  -11.2 | Total:   79.3\n",
            "Step 125 | X:  -0.24 | Y:  -0.02 | Vel: (-0.24, 0.14) | Angle:  0.48 | Reward:  -13.8 | Total:   48.8\n",
            "â­• Engines off\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "Step 140 | X:  -0.30 | Y:   0.05 | Vel: (-0.43, 0.10) | Angle:  0.58 | Reward:    0.9 | Total:   14.7\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step 160 | X:  -0.41 | Y:   0.05 | Vel: (-0.77, 0.11) | Angle:  0.29 | Reward:   -0.7 | Total:   -4.4\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "Step 180 | X:  -0.60 | Y:   0.21 | Vel: (-0.94, 0.44) | Angle: -0.12 | Reward:   -4.1 | Total:  -41.4\n",
            "ğŸ”¥ RIGHT ENGINE FIRING!\n",
            "â­• Engines off\n",
            "Step 200 | X:  -0.77 | Y:   0.46 | Vel: (-0.74, 0.46) | Angle: -0.71 | Reward:   -2.9 | Total: -113.6\n",
            "ğŸš€ MAIN ENGINE FIRING!\n",
            "â­• Engines off\n",
            "Step 220 | X:  -0.87 | Y:   0.62 | Vel: (-0.48, 0.10) | Angle: -1.34 | Reward:   -3.1 | Total: -157.9\n",
            "Step 240 | X:  -0.97 | Y:   0.55 | Vel: (-0.48,-0.43) | Angle: -1.97 | Reward:   -4.9 | Total: -241.0\n",
            "ğŸ”š Closing environment...\n",
            "âœ… Cleanup complete!\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import pygame\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Initialize pygame properly\n",
        "pygame.init()\n",
        "\n",
        "# Initialize environment with human rendering\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "obs, _ = env.reset()\n",
        "\n",
        "print(\"ğŸš€ LunarLander Manual Control Started!\")\n",
        "print(\"Goal: Land the spacecraft safely between the flags!\")\n",
        "print(\"Controls:\")\n",
        "print(\"  â† Left Arrow  = Fire left engine (rotate right)\")\n",
        "print(\"  â†’ Right Arrow = Fire right engine (rotate left)\")\n",
        "print(\"  â†‘ Up Arrow    = Fire main engine (thrust up)\")\n",
        "print(\"  â†“ Down Arrow  = Do nothing\")\n",
        "print(\"  ESC or Close Window = Quit\")\n",
        "print(\"  R = Reset episode\")\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ’¡ Tips:\")\n",
        "print(\"  - Use main engine to slow descent\")\n",
        "print(\"  - Use side engines to control rotation and horizontal movement\")\n",
        "print(\"  - Land gently between the yellow flags!\")\n",
        "print(\"  - Legs must touch ground first for safe landing\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Map keys to actions for LunarLander\n",
        "# LunarLander actions: 0=nothing, 1=fire left, 2=fire main, 3=fire right\n",
        "KEY_TO_ACTION = {\n",
        "    pygame.K_DOWN: 0,   # Do nothing\n",
        "    pygame.K_LEFT: 1,   # Fire left engine\n",
        "    pygame.K_UP: 2,     # Fire main engine\n",
        "    pygame.K_RIGHT: 3,  # Fire right engine\n",
        "}\n",
        "\n",
        "clock = pygame.time.Clock()\n",
        "running = True\n",
        "action = 0  # Start with 'do nothing'\n",
        "step_count = 0\n",
        "episode_count = 1\n",
        "total_reward = 0\n",
        "\n",
        "try:\n",
        "    while running:\n",
        "        # Process events\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "            elif event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_ESCAPE:\n",
        "                    running = False\n",
        "                elif event.key == pygame.K_r:\n",
        "                    print(\"ğŸ”„ Manual reset requested\")\n",
        "                    obs, _ = env.reset()\n",
        "                    step_count = 0\n",
        "                    total_reward = 0\n",
        "                    episode_count += 1\n",
        "                    print(f\"ğŸ†• Episode {episode_count} started\")\n",
        "\n",
        "        # Get currently pressed keys\n",
        "        keys = pygame.key.get_pressed()\n",
        "        \n",
        "        # Determine action based on keys (allow multiple keys)\n",
        "        old_action = action\n",
        "        if keys[pygame.K_UP]:\n",
        "            action = 2  # Main engine has priority\n",
        "            if old_action != action:\n",
        "                print(\"ğŸš€ MAIN ENGINE FIRING!\")\n",
        "        elif keys[pygame.K_LEFT]:\n",
        "            action = 1\n",
        "            if old_action != action:\n",
        "                print(\"ğŸ”¥ LEFT ENGINE FIRING!\")\n",
        "        elif keys[pygame.K_RIGHT]:\n",
        "            action = 3\n",
        "            if old_action != action:\n",
        "                print(\"ğŸ”¥ RIGHT ENGINE FIRING!\")\n",
        "        else:\n",
        "            action = 0\n",
        "            if old_action != action and old_action != 0:\n",
        "                print(\"â­• Engines off\")\n",
        "\n",
        "        # Step in the environment\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        step_count += 1\n",
        "        total_reward += reward\n",
        "\n",
        "        # Extract observation values (LunarLander has 8 observation values)\n",
        "        x_pos, y_pos, x_vel, y_vel, angle, angular_vel, left_leg, right_leg = obs\n",
        "        \n",
        "        # Print status every 20 steps or on important events\n",
        "        if step_count % 20 == 0 or terminated or truncated or abs(reward) > 10:\n",
        "            print(f\"Step {step_count:3d} | X: {x_pos:6.2f} | Y: {y_pos:6.2f} | \"\n",
        "                  f\"Vel: ({x_vel:5.2f},{y_vel:5.2f}) | Angle: {angle:5.2f} | \"\n",
        "                  f\"Reward: {reward:6.1f} | Total: {total_reward:6.1f}\")\n",
        "\n",
        "        # Give feedback on landing legs\n",
        "        if left_leg or right_leg:\n",
        "            if step_count % 10 == 0:  # Don't spam this message\n",
        "                legs_status = []\n",
        "                if left_leg:\n",
        "                    legs_status.append(\"LEFT\")\n",
        "                if right_leg:\n",
        "                    legs_status.append(\"RIGHT\")\n",
        "                print(f\"ğŸ¦µ Landing legs touching: {', '.join(legs_status)}\")\n",
        "\n",
        "        # Check for episode end\n",
        "        if terminated or truncated:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            if terminated:\n",
        "                if total_reward >= 200:\n",
        "                    print(\"ğŸ‰ EXCELLENT LANDING! Perfect score!\")\n",
        "                elif total_reward >= 100:\n",
        "                    print(\"ğŸ‰ SUCCESSFUL LANDING! Well done!\")\n",
        "                elif total_reward >= 0:\n",
        "                    print(\"ğŸ‘ SAFE LANDING! Could be smoother, but good job!\")\n",
        "                elif total_reward >= -100:\n",
        "                    print(\"ğŸ’¥ ROUGH LANDING! You survived but damaged the lander.\")\n",
        "                else:\n",
        "                    print(\"ğŸ’¥ CRASH! The lander was destroyed.\")\n",
        "            else:\n",
        "                print(\"â° Time limit reached!\")\n",
        "            \n",
        "            print(f\"ğŸ“Š Episode {episode_count} Results:\")\n",
        "            print(f\"   Final Score: {total_reward:.1f}\")\n",
        "            print(f\"   Steps taken: {step_count}\")\n",
        "            print(f\"   Final position: ({x_pos:.2f}, {y_pos:.2f})\")\n",
        "            print(f\"   Final velocity: ({x_vel:.2f}, {y_vel:.2f})\")\n",
        "            \n",
        "            # Scoring breakdown\n",
        "            if total_reward >= 200:\n",
        "                print(\"ğŸ† RATING: EXPERT PILOT\")\n",
        "            elif total_reward >= 100:\n",
        "                print(\"ğŸ¥‡ RATING: SKILLED PILOT\") \n",
        "            elif total_reward >= 0:\n",
        "                print(\"ğŸ¥ˆ RATING: COMPETENT PILOT\")\n",
        "            elif total_reward >= -100:\n",
        "                print(\"ğŸ¥‰ RATING: NOVICE PILOT\")\n",
        "            else:\n",
        "                print(\"ğŸ’€ RATING: NEEDS MORE PRACTICE\")\n",
        "            \n",
        "            print(\"=\"*50)\n",
        "            print(\"ğŸ”„ Resetting in 3 seconds... (Press R to reset immediately)\")\n",
        "            \n",
        "            # Wait for reset or user input\n",
        "            start_time = time.time()\n",
        "            reset_now = False\n",
        "            while time.time() - start_time < 3 and not reset_now:\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        running = False\n",
        "                        reset_now = True\n",
        "                    elif event.type == pygame.KEYDOWN:\n",
        "                        if event.key == pygame.K_ESCAPE:\n",
        "                            running = False\n",
        "                            reset_now = True\n",
        "                        elif event.key == pygame.K_r:\n",
        "                            reset_now = True\n",
        "                \n",
        "                clock.tick(60)  # Check events frequently during wait\n",
        "            \n",
        "            if running:\n",
        "                obs, _ = env.reset()\n",
        "                step_count = 0\n",
        "                total_reward = 0\n",
        "                episode_count += 1\n",
        "                print(f\"ğŸ†• Episode {episode_count} started\")\n",
        "\n",
        "        # Control frame rate\n",
        "        clock.tick(30)  # 30 FPS for smooth control\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nğŸ›‘ Interrupted by user (Ctrl+C)\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Runtime error: {e}\")\n",
        "finally:\n",
        "    print(\"ğŸ”š Closing environment...\")\n",
        "    env.close()\n",
        "    pygame.quit()\n",
        "    print(\"âœ… Cleanup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Learning Summary\n",
        "\n",
        "Congratulations! You've completed the LunarLander RL basics tutorial. Let's review what you've learned:\n",
        "\n",
        "### ğŸ§  Key Concepts Mastered:\n",
        "\n",
        "1. **ğŸ—ï¸ MDP Components in Practice:**\n",
        "   - **States**: 8-dimensional continuous space (position, velocity, angle, etc.)\n",
        "   - **Actions**: 4 discrete actions (engines: none, left, main, right)\n",
        "   - **Rewards**: Continuous rewards based on landing performance\n",
        "   - **Transitions**: Physics-based state changes\n",
        "\n",
        "2. **ğŸ² Baseline Performance:**\n",
        "   - Random agents perform poorly (~0-20% success rate)\n",
        "   - Demonstrates the need for intelligent decision-making\n",
        "   - Shows the challenge of the environment\n",
        "\n",
        "3. **ğŸ® Human Intelligence:**\n",
        "   - Manual control significantly outperforms random actions\n",
        "   - Demonstrates the value of strategy and planning\n",
        "   - Shows what RL algorithms should aspire to achieve\n",
        "\n",
        "### ğŸš€ Next Steps in Your RL Journey:\n",
        "\n",
        "1. **ğŸ“š Algorithm Learning:**\n",
        "   - Q-Learning for discrete environments\n",
        "   - Policy Gradient methods for continuous control\n",
        "   - Deep RL for complex state spaces\n",
        "\n",
        "2. **ğŸ› ï¸ Implementation Skills:**\n",
        "   - Building RL agents from scratch\n",
        "   - Training and evaluation pipelines\n",
        "   - Hyperparameter tuning\n",
        "\n",
        "3. **ğŸ¯ Advanced Topics:**\n",
        "   - Multi-agent RL\n",
        "   - Transfer learning\n",
        "   - Real-world applications\n",
        "\n",
        "### ğŸ† Achievement Unlocked:\n",
        "- âœ… Environment Analysis Expert\n",
        "- âœ… Agent Implementation Basics\n",
        "- âœ… Performance Evaluation Skills\n",
        "\n",
        "\n",
        "**Ready for the next challenge? Let's dive into RL algorithms! ğŸš€**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ… Bonus Challenges\n",
        "\n",
        "Want to explore further? Try these challenges:\n",
        "\n",
        "### ğŸ¯ Challenge 1: Heuristic Agent\n",
        "Create a simple heuristic agent that uses basic rules:\n",
        "- Fire main engine when falling too fast\n",
        "- Use side engines to control rotation\n",
        "- Coast when trajectory looks good\n",
        "\n",
        "### ğŸ¯ Challenge 2: Environment Variations\n",
        "Try different LunarLander variants:\n",
        "- `LunarLander-v2` (standard)\n",
        "- `LunarLanderContinuous-v2` (continuous actions)\n",
        "- Different gravity settings\n",
        "\n",
        "### ğŸ¯ Challenge 3: Data Collection\n",
        "Collect and analyze data from your manual play:\n",
        "- State trajectories\n",
        "- Action sequences\n",
        "- Reward patterns\n",
        "- Success factors\n",
        "\n",
        "### ğŸ¯ Challenge 4: Visualization\n",
        "Create visualizations of:\n",
        "- Landing trajectories\n",
        "- Action usage patterns\n",
        "- Learning curves\n",
        "- State space exploration\n",
        "\n",
        "### ğŸ¯ Challenge 5: Mini RL Algorithm\n",
        "Implement a simple learning algorithm:\n",
        "- Tabular Q-learning (discretize states)\n",
        "- Simple policy gradient\n",
        "- Behavioral cloning from your manual play\n",
        "\n",
        "**Good luck, future RL engineer! ğŸš€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
